---
layout: page
title: General Matrix Multiply
subtitle: UCSD, California | Apr - Jun 2022
background_image: parallel_computation.jpeg
---

<!-- Header -->
<header id="header">
	<h1><a href="/site/index.html">Raghav Prasad</a></h1>
</header>

<section>
	<h4>General Matrix Multiplication</h4>
	<p>
		Matrix multiplication is a really frequent operation performed in various computer applications (ML and graphics processing, to name a couple). The naive matrix multiplication algorithm runs in O(n^3) and is really inefficient in terms of leveraging computational and memory resources. This project implements matrix multiplication modeled after <a href="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">CUTLASS</a> and achieves 60.86% of the performance benchmark of cuBLAS.<br>
	</p>
	<header>
		<h4>Dissecting GEMM</h4>
	</header>
	<p>
		My GEMM program is written using CUDA, and runs on an Nvidia Kepler K80. My program makes use of the hardware advantages presented by GPU architecture, such as fast shared memory, multiple threads (TLP - thread-level parallelism), and a large number of cores.<br>
		In addition to thread-level parallelism, my program also implements instruction-level parallelism (ILP) and vectorization to boost performance. This yielded a performance of 621.7 GFlops. <br>
	</p>

	<a href="https://github.com/cse260-sp22/gemm-gpu" target="_blank" class="button special" style="margin-right: 450px; margin-left: 450px; display: inline-block; text-align: center">Check it out</a>

</section>